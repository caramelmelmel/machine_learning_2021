{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import part4\n",
    "from copy_utilities import read_universal_\n",
    "import utilities\n",
    "import part2\n",
    "import part1\n",
    "import os \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all file paths\n",
    "#common words are stop words for each language\n",
    "STOP_words_ES_file_path = read_universal_('stopwords_ES.txt','ES')\n",
    "STOP_words_RU_file_path = read_universal_('stopwords_RU.txt','RU')\n",
    "\n",
    "#training path \n",
    "RU_train = read_universal_('train','RU')\n",
    "ES_train = read_universal_('train','ES')\n",
    "\n",
    "# test file path dev set\n",
    "RU_test_dev = read_universal_('dev.in','RU')\n",
    "ES_test_dev = read_universal_('dev.in','RU')\n",
    "\n",
    "# write out file path\n",
    "RU_test_dev_write = read_universal_('dev.p4.out','RU')\n",
    "ES_test_write = read_universal_('dev.p4.out','ES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read stopwords\n",
    "#return nested lsit of stop words\n",
    "ES_stopwords = part4.read_stopwords(STOP_words_ES_file_path)\n",
    "RU_stopwords = part4.read_stopwords(STOP_words_RU_file_path)\n",
    "print (RU_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all the russian and spanish stopwords\n",
    "def remove_stopwords(dataset,stop_words,symbols_list=[]):\n",
    "    original_dataset = []\n",
    "    file_dataset = open(dataset,'r',encoding='utf-8')\n",
    "    training_set = file_dataset.readlines()\n",
    "    for line in training_set:\n",
    "        # if to include \\n\n",
    "        if len(line) == 1:\n",
    "            original_dataset.append(\"\\n\")\n",
    "        else:\n",
    "            content_line = line.split() #split into the \"text\" and the \"tag/label\" using line.split()\n",
    "            if content_line[0] in stop_words: #if the word belongs to the list of stopwords or list of symbols, assign the label O\n",
    "                content_line[1] = \"O\" #assign label O\n",
    "                original_dataset.append(content_line) #append to the ES dataset\n",
    "            elif content_line[0] in symbols_list:\n",
    "                continue\n",
    "                #print(content_line[0])\n",
    "        #Remove empty lists within list\n",
    "    Edataset = [ele for ele in original_dataset]\n",
    "    return Edataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RU_wo_stopwords = remove_stopwords(RU_train,RU_stopwords)\n",
    "print(RU_wo_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_train = utilities.read_data_transmission(ES_train)\n",
    "tags = utilities.count_tags_transmission(es_train)\n",
    "tag_words = utilities.count_tag_words(es_train)\n",
    "transmission_counts = part2.count_transmissions(es_train)\n",
    "#print(transmission_counts)\n",
    "t_params = part2.estimate_transmission_parameters(transmission_counts, tags)\n",
    "print(t_params)\n",
    "#e_params = estimate_emission_parameters_with_unk(tags, tag_words)\n",
    "print(\"=======================\")\n",
    "\n",
    "stopwordsRU = part4.read_stopwords(STOP_words_RU_file_path)\n",
    "modif_RU = part4.remove_stopwords_es(RU_train,stopwordsRU)\n",
    "tags_ru = utilities.count_tags_transmission(modif_RU)\n",
    "print(modif_RU)\n",
    "print(part4.get_symbols(modif_RU))\n",
    "\n",
    "\n",
    "# Label Smoothing\n",
    "# https://www.pyimagesearch.com/2019/12/30/label-smoothing-with-keras-tensorflow-and-deep-learning/\n",
    "#may need to modify for PART IV, also different cases for start/stop possibly\n",
    "#TODO modify this\n",
    "def smooth_labels_transmission(transmission_parameters,alpha_sm=0.1):\n",
    "    smoothed_dict = transmission_parameters\n",
    "    #alpha_sm = 0.1 #hyperparameter for label smoothing, default 0.1\n",
    "    for entry in smoothed_dict: #for each dictionary\n",
    "        for label in smoothed_dict[entry]: #for each label in each dictionary\n",
    "            if label != 'START':\n",
    "                smoothed_dict[entry][label] *= (1 - alpha_sm)\n",
    "                smoothed_dict[entry][label] += (alpha_sm/(len(smoothed_dict[entry])-1)) #add by the smoothing factor/number of labels without START\n",
    "            # there is no transition to START at all\n",
    "    return smoothed_dict #return smoothed dictionary\n",
    "\n",
    "print('The smoothed dictionary is below \\n')\n",
    "\n",
    "transition_with_smooth = smooth_labels_transmission(t_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing do the\n",
    "#emission dictionary takes the form of the word-> label\n",
    "test_RU_dev= utilities.read_dev(RU_test_dev)\n",
    "e_params = part1.estimate_emission_parameters_with_unk(tags,tag_words)\n",
    "\n",
    "def smooth_labels_emission(emission_dict,alpha_sm=0.1):\n",
    "    smoothed_dict = emission_dict\n",
    "    for label, word_prob_dict in emission_dict.items():\n",
    "        for word in word_prob_dict.keys():\n",
    "            smoothed_dict[label][word] *= (1 - alpha_sm)\n",
    "            smoothed_dict[label][word] += (alpha_sm/(len(word_prob_dict)))\n",
    "    return smoothed_dict\n",
    "\n",
    "emission_portion = smooth_labels_emission(e_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viterbi output\n",
    "def run_viterbi(training_path, test_path, output_path,alpha_sm,stop_words_file_path):\n",
    "    train = utilities.read_data_transmission(training_path)\n",
    "    #read the stopwords\n",
    "    stop_word = part4.read_stopwords(stop_words_file_path)\n",
    "    train =  remove_stopwords(training_path,stop_word)\n",
    "    train_words = utilities.get_training_set_words(train)\n",
    "    test = utilities.read_dev(test_path)\n",
    "    tags = utilities.count_tags_transmission(train)\n",
    "    tag_words = utilities.count_tag_words(train)\n",
    "    transmission_counts = part2.count_transmissions(train)\n",
    "    t_params = part2.estimate_transmission_parameters(transmission_counts, tags)\n",
    "    t_params = smooth_labels_transmission(t_params,alpha_sm=alpha_sm)\n",
    "    e_params = part1.estimate_emission_parameters_with_unk(tags, tag_words)\n",
    "    e_params = smooth_labels_emission(e_params,alpha_sm=alpha_sm)\n",
    "    prediction = part2.viterbi_loop(test, t_params, e_params, train_words)\n",
    "    utilities.output_prediction(prediction, test, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothing where alpha = 0.1\n",
    "\n",
    "!python3 EvalScript/evalResult.py RU/dev.out RU/dev.p4.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.3\n",
    "for i in np.arange(0,a,0.005):\n",
    "    if os.path.exists(RU_test_dev_write):\n",
    "        os.remove(RU_test_dev_write)\n",
    "    run_viterbi(RU_train,RU_test_dev,RU_test_dev_write,i)\n",
    "    stream = os.popen('python3 EvalScript/evalResult.py RU/dev.out RU/dev.p4.out')\n",
    "    output = stream.read()\n",
    "    print(f'When smoothing parameter = {i}')\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_file_RU = read_universal_('test.in','test/RU-test')\n",
    "print(TEST_file_RU)\n",
    "run_viterbi(RU_train,TEST_file_RU,'RU/test.p4.out',0.01)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
